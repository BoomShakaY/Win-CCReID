
apex: false                      # Set True to use float16.
B_w: 0.2                         # The loss weight of fine-grained loss, which is named as `alpha` in the paper.

#ID_class: 751
#ID_class: 150                    # The number of ID classes in the dataset. For example, 751 for Market, 702 for DukeMTMC, 150 for PRCC, 77 for LTCC
ID_class: 77

ID_stride: 1                     # Stride in Appearance encoder

#ID_style: AB
ID_style: ACID
soft: false                       # weather use Soft CA + channel
treshold_channel: 64


CA: true
cau_w: 1
cau_pw: 1
cau_cw: 1
cau_recon_w: 1


dataset: ltcc                    # Dataset optional 'prcc' or 'ltcc' or 'vc' or 'celeb' or 'celeb_light'
data_root: YOUR_DATA_PATH/LTCC_ReID/pytorch    # Dataset Root

batch_size: 8                    # Origin BatchSize

use_cloth: true                  # Weather use the cloth-relative datasets

ibn: false
circle: false
circle_w: 1

CA_layer: 4
CA_ALL : false                    # Weather to use the CA in the reconstructed Images

dist_style : euclidean              # The distance calculate by 'cosine' or 'euclidean', e is better
margin: 0.5                      # The margin of the Triplet loss

pid_w: 1.0                       # positive ID loss
cid_w: 1.0                       # cloth ID loss
#cid_w: 0.0                       # do not use cloth-branch
#trip_id_w : 0.0                  #
#trip_pid_w : 0.0                 #
trip_id_w : 1.0                  # Triplet loss weight for id
trip_pid_w : 1.0                 # Triplet loss weight for pid

beta1: 0                         # Adam hyperparameter
beta2: 0.999                     # Adam hyperparameter
crop_image_height: 256           # Input height
crop_image_width: 128            # Input width

dis:
  LAMBDA: 0.01                   # the hyperparameter for the regularization term
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dim: 32                        # number of filters in the bottommost layer
  gan_type: lsgan                # GAN loss [lsgan/nsgan]
  n_layer: 2                     # number of layers in D
  n_res: 4                       # number of layers in D
  non_local: 0                   # number of non_local layers
  norm: none                     # normalization layer [none/bn/in/ln]
  num_scales: 3                  # number of scales
  pad_type: reflect              # padding type [zero/reflect]
display_size: 16                 # How much display images
erasing_p: 0.5                   # Random erasing probability [0-1]
gamma: 0.1                       # Learning Rate Decay (except appearance encoder)
gamma2: 0.1                      # Learning Rate Decay (for appearance encoder)
gan_w: 1                         # the weight of gan loss
gen:
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dec: basic                     # [basic/parallel/series]
  dim: 16                        # number of filters in the bottommost layer
  dropout: 0                     # use dropout in the generator
  id_dim: 2048                   # length of appearance code
  mlp_dim: 512                   # number of filters in MLP
  mlp_norm: none                 # norm in mlp [none/bn/in/ln]
  n_downsample: 2                # number of downsampling layers in content encoder
  n_res: 4                       # number of residual blocks in content encoder/decoder
  non_local: 0                   # number of non_local layer
  pad_type: reflect              # padding type [zero/reflect]
  tanh: false                    # use tanh or not at the last layer
  init: kaiming                  # initialization [gaussian/kaiming/xavier/orthogonal]
id_w: 1.0                        # the weight of ID loss

image_display_iter: 10000         # How often do you want to display output images during training
image_save_iter: 10000            # How often do you want to save output images during training
#image_display_iter: 1000         # How often do you want to display output images during training
#image_save_iter: 1000            # How often do you want to save output images during training

input_dim_a: 1                   # We use the gray-scale input, so the input dim is 1
input_dim_b: 1                   # We use the gray-scale input, so the input dim is 1
log_iter: 1                      # How often do you want to log the training stats
lr2: 0.002                       # initial appearance encoder learning rate
lr_d: 0.0001                     # initial discriminator learning rate
lr_g: 0.0001                     # initial generator (except appearance encoder) learning rate
lr_policy: multistep             # learning rate scheduler [multistep|constant|step]
max_cyc_w: 2                     # the maximum weight for cycle loss

#max_iter: 100000                 # When you end the training
max_iter: 74000                 # When you end the training  2237 images 61epoch

max_teacher_w: 2                 # the maximum weight for prime loss (teacher KL loss)
max_w: 1                         # the maximum weight for feature reconstruction losses
new_size: 128                    # the resized size
norm_id: false                   # Do we normalize the appearance code
num_workers: 4                   # nworks to load the data

#pid_w: 1.0                       # positive ID loss
#cid_w: 1.0                       # cloth ID loss
#trip_id_w : 1.0                  # Triplet loss weight for id
#trip_pid_w : 1.0                 # Triplet loss weight for pid

pool: max                        # pooling layer for the appearance encoder
recon_s_w: 0                     # the initial weight for structure code reconstruction
recon_f_w: 0                     # the initial weight for appearance code reconstruction
recon_id_w: 0.5                  # the initial weight for ID reconstruction
recon_x_cyc_w: 0                 # the initial weight for cycle reconstruction
recon_x_w: 5                     # the initial weight for self-reconstruction
recon_xp_w: 5                    # the initial weight for self-identity reconstruction
single: gray                     # make input to gray-scale

#snapshot_save_iter: 10000        # How often to save the checkpoint
snapshot_save_iter: 7400        # How often to save the checkpoint

sqrt: false                      # whether use square loss.

#step_size: 60000                 # when to decay the learning rate
step_size: 44000                 # when to decay the learning rate

#teacher: prcc2_warm5_s1_b8_lr2_p0.5                     # teacher model name. For DukeMTMC, you may set `best-duke`
teacher: ltcc_warm5_s1_b8_lr2_p0.5

teacher_w: 0                     # the initial weight for prime loss (teacher KL loss)
teacher_style: 0                 # select teacher style.[0-4] # 0: Our smooth dynamic label# 1: Pseudo label, hard dynamic label# 2: Conditional label, hard static label # 3: LSRO, static smooth label# 4: Dynamic Soft Two-label
train_bn: true                   # whether we train the bn for the generated image.
use_decoder_again: true          # whether we train the decoder on the generatd image.
use_encoder_again: 0.5           # the probability we train the structure encoder on the generatd image.
vgg_w: 0                         # We do not use vgg as one kind of inception loss.

#warm_iter: 30000                 # when to start warm up the losses (fine-grained/feature reconstruction losses).
warm_iter: 22000                 # when to start warm up the losses (fine-grained/feature reconstruction losses).
#warm_iter: 1                     # TEST when to start warm up the losses (fine-grained/feature reconstruction losses).

warm_scale: 0.0005               # how fast to warm up

#warm_teacher_iter: 30000         # when to start warm up the prime loss
warm_teacher_iter: 22000         # when to start warm up the prime loss
#warm_teacher_iter: 1         # TEST when to start warm up the prime loss

weight_decay: 0.0005             # weight decay
